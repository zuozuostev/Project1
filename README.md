# Project1
## 机器学习与数据挖掘实验文件库
组员信息
    李华辉（组长）   冼海俊
    
    
   ## 实验一
    ###分工信息
	李华辉：代码的实现部分+文档编写（80%）
	冼海俊：代码的规范化+文档编写+作业提交（20%）
### 作业题目和内容
	题目：多源数据集成、清洗和统计
	内容：广州大学某班有同学100人，现要从两个数据源汇总学生数据。第一个数据源在数据库中，第二个数据源在txt文件中，两个数据源课程存在缺失、冗余和不一致性， 请用C/C++/Java程序实现对两个数据源的一致性合并以及每个学生样本的数值量化。两个数据源合并后读入内存，并统计：
1、学生中家乡在Beijing的所有课程的平均成绩。
2、学生中家乡在广州，课程1在80分以上，且课程10在9分以上的男同学的数量。
3、比较广州和上海两地女生的平均体能测试成绩，哪个地区的更强些？
4、学习成绩和体能测试成绩，两者的相关性是多少？

### 作业环境
#### 文件说明：一个python 文件 main.py
#### 函数说明：
   自己编写了一个计算相关性的函数 cor(a,b)   调用了计算开根号的函数math.sqrt()，计算列表所有元素和的sum()，计算列表长度的len()，导入了pandas包，使用了它的导入数据源不同的函数  .read_excel（）和.read_csv（），数据源去重函数 drop_duplicates（），多数据源合并函数merge（），空值替换函数  fillna（），本次实验主要涉及的主要技术为pandas 里面的dataframe 的相关数据处理操作。

### 难题与解决
	在刚开始的时候不知道如何对数据进行操作，然后去了解了pandas库之后，学习了许多对数操作的方法。
	刚开始不知道以什么标准去合并两个数据源，后来观察两个数据源的ID发现，他们的差值都是202000，所以就可以统一ID，以ID为唯一标准去合并两个数据源，运用数据库的外连接思想。

### 总结
	通过本次实验，对多源数据源的合并、处理、清理有了初步的认识，对pandas库也有了初步的了解，掌握了数据清洗的具体步骤和套路。



## 实验二
### 分工信息：
	李华辉：代码的实现部分+文档编写（80%）
	冼海俊：代码的规范化+文档编写+作业提交（20%）
### 题目
基于实验一中清洗后的数据练习统计和视化操作，100个同学（样本），每个同学有11门课程的成绩（11维的向量）；那么构成了一个100x11的数据矩阵。以你擅长的语言C/C++/Java/Python/Matlab，编程计算：
1.	请以课程1成绩为x轴，体能成绩为y轴，画出散点图。
2.	以5分为间隔，画出课程1的成绩直方图。
3.	对每门成绩进行z-score归一化，得到归一化的数据矩阵。
4.	计算出100x100的相关矩阵，并可视化出混淆矩阵。（为避免歧义，这里“协相关矩阵”进一步细化更正为100x100的相关矩阵，100为学生样本数目，视实际情况而定）
5.	根据相关矩阵，找到距离每个样本最近的三个样本，得到100x3的矩阵（每一行为对应三个样本的ID）输出到txt文件中，以\t,\n间隔。
### 作业环境
#### 文件说明：
   编写了两个exe文件，配合main函数运行，exe1包括对学生成绩的整理以及相关系数的计算，exe2是对学生成绩数据的归一化及可视化处理。
#### 函数说明：
exe1中 
  #相关系数计算cor(a, b)
在show_exe1(d, num)函数中，实现：
 1.	学生中家乡在Beijing的所有课程的平均成绩。
 2.	学生中家乡在广州，课程1在80分以上，且课程9在9分以上的男同学的数量。
3.	比较广州和上海两地女生的平均体能测试成绩，哪个地区的更强些
  4．学习成绩和体能测试成绩，两者的相关性是多少
exe2中
draw_scatter(d)	：画散点图
draw_hist(d):：画直方图
z_score(d, num)：数据归一化
stu_cor(z_d, num):计算学生的相关矩阵
find_ID(d, d_stu, num): 根据相关矩阵，找到距样本最离每个近的三个样本，得到100x3的矩阵
### 难题与解决
做到可视化混淆矩阵时，就搞不懂什么是混淆矩阵。最后通过和同组组员学习讨论，知道就是可视化出他的热力图。同时，python也有相关的库可以调用。问题就得到解决了。

还有另一个难题就是，我们想了好久都不知道如何从相关矩阵中去找出3个最接近的样本。后来通过讨论和观察，发现可以通过对应的相关系数对应的index来关联相关矩阵和处理好的数据集。
### 总结
通过本次实验，我初步了解python的画图函数，也体会到它的强大、方便之处。对数据有了进一步处理，比如归一化，标准化。以及学会了一本文件的读取和写入的操作。


## 实验三
分工信息
	李华辉：代码的实现部分+文档编写（80%）
	冼海俊：代码的规范化+文档编写+作业提交（20%）
### 题目
用C++实现k-means聚类算法，
1.	对实验二中的z-score归一化的成绩数据进行测试，观察聚类为2类，3类，4类，5类的结果，观察得出什么结论？
2.	由老师给出测试数据，进行测试，并画出可视化出散点图，类中心，类半径，并分析聚为几类合适。
样例数据(x,y)数据对：
3.45	7.08
1.76	7.24
4.29	9.55
3.35	6.65
3.17	6.41
3.68	5.99
2.11	4.08
2.58	7.10
3.45	7.88
6.17	5.40
4.20	6.46
5.87	3.87
5.47	2.21
5.97	3.62
6.24	3.06
6.89	2.41
5.38	2.32
5.13	2.73
7.26	4.19
6.32	3.62
找到聚类中心后，判断(2,6)是属于哪一类？
注意
除文件读取外，不能使用C++基础库以外的API和库函数。
作业环境：
文件说明：
一个pycharm文件exe3，两个TXT文件
    Lee.txt:  经过实验二处理得出的数据。
    Teacher.txt:  教师指定数据集
函数说明：
  #计算欧式距离
def euclDistance(vector1, vector2):
    dist = np.sqrt(sum(pow(vector2 - vector1, 2)))
    return dist
  #使用随机样例初始化质心
def findPoints(dataSet, k):
    numSamples, dim = dataSet.shape  #计算数据集的行列

    centroids = np.zeros((k, dim))  #产生k行，dim列零矩阵
    for i in range(k):
        index = int(np.random.uniform(0, numSamples)) #给出一个服从均匀分布的在0~numSamples之间的整数
        centroids[i, :] = dataSet[index, :]  # 第index行作为簇心
    return centroids
#k均值聚类
def kmeans(dataSet, k)
#1.对实验二中的z-score归一化的成绩数据进行测试
    #源数据获取
    df = pd.read_table("Lee_data.txt", header=None, sep='\t')
    df = df.sample(frac=1)   / 打乱数据，使之更具客观性
#2、由老师给出测试数据，进行测试，并画出可视化出散点图，类中心，类半径，并分析聚为几类合适
    # 源数据获取
    df = pd.read_table("teacher_data.txt", header=None, sep='\t')
    df.columns = ['x', 'y']
    # 画散点图
    plt.xlabel("x")  # 定义x坐标
plt.ylabel("y")   定义y坐标

### 难题与解决
### 总结：
本次实验对老师给的数据可以得出较好的聚类效果。但是不知道如何计算类的半径。通过本次实验，我都是有很大的收获的。学习了k-means聚类算法的原理，发现其实也不难。当然，还有很多其他优秀的聚类算法需要我一一去学习。


## 实验四
分工信息
	李华辉：代码的实现部分+文档编写（80%）
	冼海俊：代码的规范化+文档编写+作业提交（20%）

### 题目
学习sigmoid函数和逻辑回归算法。将实验三.2中的样例数据用聚类的结果打标签{0，1}，并用逻辑回归模型拟合。
1.	学习并画出sigmoid函数
2.	设计梯度下降算法，实现逻辑回归模型的学习过程。
3.	根据给定数据（实验三.2），用梯度下降算法进行数据拟合，并用学习好的模型对(2,6)分类。
（对2,3实现有难度的同学，可以直接调用sklearn中LogisticRegression进行学习）

### 作业环境：
■文件说明：
一个pycharm文件exe4，与main函数配合运行
■函数说明：
  #导入逻辑回归模型函数库
from sklearn.linear_model import LogisticRegression

#训练效果可视化
def train_plt(data, label, lr_clf):

plt_sigmoid(data, w):
    # 其拟合方程形式为f(x)=w0+w1*x1+w2*x2....
    # 生成两个矩阵
    data = np.mat(data)
    w = np.array(w)
    z = data * np.transpose(w)  # 矩阵转置，相乘后得到z值
    z.sort(axis=0)

    # 画sigmoid函数
    phi_z = sigmoid(z)
    plt.plot(z, phi_z)
    plt.axvline(0.0, color='k')
    plt.axhspan(0.0, 1.0, facecolor='1.0', alpha=1.0, ls='dotted')
    plt.yticks([0.0, 0.5, 1.0])
    plt.ylim(-0.1, 1.1)
    plt.xlabel('z')
    plt.ylabel('$\phi (z)$')
plt.show()
#可视化预测新样本
def test_plt(data, label, lr_clf)
### 难题与解决
### 总结
实验3得到了数据的分类结果，先把这些结果一一对应的添加到老师给的数据上。形成了带有标签的数据源。
然后调用逻辑回归模型，并拟合所构造的数据集。sigmoid函数（phi_z = 1.0 / (1.0 + np.exp(-z))，得到x轴、y轴的数据，即可画出sigmoid函数。最后可视化训练结果（用一条直线划分数据为两类）、可视化对（2，6）的分类结果。
通过本次实验，了解了并学习sigmoid函数，知道了它的作用。但是本次实验是调用python里面的sk-learn库现成的逻辑回归模型，没有能够用自己的代码实现。但是我也去网上学习了这个模型。也有不少的收获。
